{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import implementations as m\n",
    "np.random.seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    centered_data = x - np.mean(x, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    \n",
    "    return std_data,np.mean(x, axis=0),np.std(centered_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_nan(tX):\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if(tX[i,j]==-999):\n",
    "                tX[i,j]=np.nan\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_other_number(tX, new_value):\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if(tX[i,j]==-999):\n",
    "                tX[i,j]=new_value\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_median(tX):\n",
    "    median_per_col=np.nanmedian(tX,axis=0)\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(len(median_per_col)):\n",
    "            if(np.isnan(tX[i,j])):\n",
    "                tX[i,j]=median_per_col[j]\n",
    "    return tX,median_per_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_transform(x,min_,max_):\n",
    "    return (x-min_)/(max_-min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_and_normalize_X(X,d):\n",
    "    \"\"\"\n",
    "    perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\n",
    "    and normalize them.\n",
    "    \"\"\"\n",
    "\n",
    "    expand = build_poly(X,d)\n",
    "    expand_withoutBias,mu,std = normalize(expand[:,1:])\n",
    "    expand[:,1:] = expand_withoutBias\n",
    "    return expand, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_pred==y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    ones_col = np.ones((len(x), 1))\n",
    "    poly = x\n",
    "    m, n = x.shape\n",
    "    for deg in range(2, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    multi_indices = {}\n",
    "    cpt = 0\n",
    "    for i in range (n):\n",
    "        for j in range(i+1,n):\n",
    "            multi_indices[cpt] = [i,j]\n",
    "            cpt = cpt+1\n",
    "    \n",
    "    gen_features = np.zeros(shape=(m, len(multi_indices)) )\n",
    "\n",
    "    for i, c in multi_indices.items():\n",
    "        gen_features[:, i] = np.multiply(x[:, c[0]],x[:, c[1]])\n",
    "\n",
    "    poly =  np.c_[poly,gen_features]\n",
    "    poly =  np.c_[ones_col,poly]\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX=number_to_nan(tX)\n",
    "tX0=[]\n",
    "tX1=[]\n",
    "tX2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, jet_num in enumerate(tX[:,22]):\n",
    "    if(int(jet_num)==0):\n",
    "        tX0.append(np.insert(tX[index],0,index))\n",
    "    if(int(jet_num)==1):\n",
    "        tX1.append(np.insert(tX[index],0,index))\n",
    "    if(int(jet_num)==2 or int(jet_num)==3):\n",
    "        tX2.append(np.insert(tX[index],0,index))\n",
    "tX0=np.array(tX0)\n",
    "tX1=np.array(tX1)\n",
    "tX2=np.array(tX2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tx0 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0_dropped=np.delete(tX0,[5,6,7,13,24,25,26,27,28,29],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0_dropped[:,1:2], median0=nan_to_median(tX0_dropped[:,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0_dropped_distribution=np.delete(tX0_dropped,[3,4,7,13,19,20],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_be_skewed0=[2,7,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tx1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX1_dropped=np.delete(tX1,[5,6,7,13,27,28,29],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX1_dropped[:,1:2], median1=nan_to_median(tX1_dropped[:,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX1_dropped_distribution=np.delete(tX1_dropped,[4,7,19,20],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_be_skewed1=[2,4,8,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tx2 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX2[:,1:2], median2=nan_to_median(tX2[:,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX2_dropped_distribution=np.delete(tX2,[4,6,22,23,24,27,30],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_be_skewed2=[2,8,9,12,15,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the skew Indexes\n",
    "index_to_be_skewed0=(index_to_be_skewed0-np.ones(len(index_to_be_skewed0))).astype(int)\n",
    "index_to_be_skewed1=(index_to_be_skewed1-np.ones(len(index_to_be_skewed1))).astype(int)\n",
    "index_to_be_skewed2=(index_to_be_skewed2-np.ones(len(index_to_be_skewed2))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices,k, degree,index_to_be_skewed):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    x_train = x[np.array([p for i in range(k_indices.shape[0]) if i != k for p in k_indices[i]])]\n",
    "    y_train= y[np.array([p for i in range(k_indices.shape[0]) if i != k for p in k_indices[i]])]\n",
    "    \n",
    "    x_test=x[k_indices[k]]\n",
    "    y_test=y[k_indices[k]]\n",
    "    \n",
    "    min_tr=np.min(x_train,axis=0)\n",
    "    max_tr=np.max(x_train,axis=0)\n",
    "    \n",
    "    ##Do all this transformation BEFORE expansion?? (both tr and te)\n",
    "    #Transformations to train\n",
    "    x_train=min_max_transform(x_train,min_tr,max_tr)\n",
    "\n",
    "    x_train[:,index_to_be_skewed]= np.log(x_train[:,index_to_be_skewed]+1)\n",
    "    x_train_poly,mean_train,std_train= expand_and_normalize_X(x_train,degree)\n",
    "    \n",
    "    #transformations to test\n",
    "    x_test= min_max_transform(x_test,min_tr,max_tr)\n",
    "    x_test[:,index_to_be_skewed]= x_test[:,index_to_be_skewed] #+np.abs(tr_skew_mins[index_to_be_skewed])#use same tr_skew_mins as before\n",
    "    x_test[:,index_to_be_skewed]= np.log(x_test[:,index_to_be_skewed]+1)\n",
    "    x_test_poly=build_poly(x_test,degree)\n",
    "    x_test_poly[:,1:]=(x_test_poly[:,1:]-mean_train)/std_train\n",
    "    \n",
    "    \n",
    "    w,loss=m.least_squares(y_train, x_train_poly)\n",
    "\n",
    "    loss_tr= -accuracy(y_train, predict_labels(w,x_train_poly))\n",
    "    loss_te= -accuracy(y_test, predict_labels(w,x_test_poly))\n",
    "    return loss_tr, loss_te,min_tr,max_tr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo(y, x,min_,max_,index_to_be_skewed):\n",
    "    seed = 1\n",
    "    k_fold = 5\n",
    "    degrees = np.arange(min_,max_)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    mse_tr = []\n",
    "    mse_te = []\n",
    "    min_loss=np.inf\n",
    "    min_degree=0\n",
    "    \n",
    "    for degree_ in degrees:\n",
    "        print(\"Current degree: \",degree_)\n",
    "        loss_tr_degree=np.array([])\n",
    "        loss_te_degree=np.array([])\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            loss_tr_k, loss_te_k,min_tr_k,max_tr_k=cross_validation(y, x, k_indices, k, degree_,index_to_be_skewed)\n",
    "            loss_tr_degree= np.append(loss_tr_degree,loss_tr_k)\n",
    "            loss_te_degree= np.append(loss_te_degree,loss_te_k)\n",
    "        \n",
    "        mse_tr.append(np.median(loss_tr_degree))\n",
    "        mse_te.append(np.median(loss_te_degree))\n",
    "        if(loss_te_degree.mean()<min_loss):\n",
    "            min_loss=loss_te_degree.mean()\n",
    "            min_degree=degree_\n",
    "            \n",
    "    return min_degree,min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting cross validation for the tx0 dataset\")\n",
    "min_degree0,min_loss0=cross_validation_demo(y[tX0_dropped_distribution[:,0].astype(int)], tX0_dropped_distribution[:,1:],1,15,index_to_be_skewed0)\n",
    "\n",
    "min0= np.min(tX0_dropped_distribution[:,1:],axis=0)\n",
    "max0=np.max(tX0_dropped_distribution[:,1:],axis=0)\n",
    "tx0=min_max_transform(tX0_dropped_distribution[:,1:],min0,max0)\n",
    "tx0[:,index_to_be_skewed0]= np.log(tx0[:,index_to_be_skewed0]+1)\n",
    "tx0_norm,mean0,std0=expand_and_normalize_X(tx0,min_degree0)\n",
    "\n",
    "\n",
    "w0,loss0=m.least_squares(y[tX0_dropped_distribution[:,0].astype(int)],tx0_norm)\n",
    "min_degree0,min_loss0,loss0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of best w found for tx0\",accuracy(y[tX0_dropped_distribution[:,0].astype(int)],predict_labels(w0,tx0_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting cross validation for the tx1 dataset\")\n",
    "min_degree1,min_loss1=cross_validation_demo(y[tX1_dropped_distribution[:,0].astype(int)], tX1_dropped_distribution[:,1:],1,16,index_to_be_skewed1)\n",
    "\n",
    "min1= np.min(tX1_dropped_distribution[:,1:],axis=0)\n",
    "max1=np.max(tX1_dropped_distribution[:,1:],axis=0)\n",
    "tx1=min_max_transform(tX1_dropped_distribution[:,1:],min1,max1)\n",
    "tx1[:,index_to_be_skewed1]= np.log(tx1[:,index_to_be_skewed1]+1)\n",
    "tx1_norm,mean1,std1=expand_and_normalize_X(tx1,min_degree1)\n",
    "\n",
    "w1,loss1=m.least_squares(y[tX1_dropped_distribution[:,0].astype(int)],tx1_norm)\n",
    "min_degree1,min_loss1,loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of best w found for tx1\",accuracy(y[tX1_dropped_distribution[:,0].astype(int)],predict_labels(w1,tx1_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting cross validation for the tx2 dataset\")\n",
    "min_degree2,min_loss2=cross_validation_demo(y[tX2_dropped_distribution[:,0].astype(int)], tX2_dropped_distribution[:,1:],1,16,index_to_be_skewed2)\n",
    "\n",
    "min2= np.min(tX2_dropped_distribution[:,1:],axis=0)\n",
    "max2=np.max(tX2_dropped_distribution[:,1:],axis=0)\n",
    "tx2=min_max_transform(tX2_dropped_distribution[:,1:],min2,max2)\n",
    "tx2[:,index_to_be_skewed2]= np.log(tx2[:,index_to_be_skewed2]+1)\n",
    "tx2_norm,mean2,std2=expand_and_normalize_X(tx2,min_degree2)\n",
    "\n",
    "w2,loss2=m.least_squares(y[tX2_dropped_distribution[:,0].astype(int)],tx2_norm)\n",
    "min_degree2,min_loss2,loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of best w found for tx2\",accuracy(y[tX2_dropped_distribution[:,0].astype(int)],predict_labels(w2,tx2_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "OUTPUT_PATH = '../data/result_LS.csv'\n",
    "\n",
    "\n",
    "x0=[]\n",
    "x1=[]\n",
    "x2=[]\n",
    "y0indices=[]\n",
    "y1indices=[]\n",
    "y2indices=[]\n",
    "\n",
    "for index, jet_num in enumerate(tX_test[:,22]):   \n",
    "    if(int(jet_num)==0):\n",
    "        x0.append(tX_test[index])\n",
    "        y0indices.append(index)\n",
    "    if(int(jet_num)==1):\n",
    "        x1.append(tX_test[index])\n",
    "        y1indices.append(index)\n",
    "    if(int(jet_num)==2 or int(jet_num)==3):\n",
    "        x2.append(tX_test[index])\n",
    "        y2indices.append(index)\n",
    "        \n",
    "x0=np.array(x0)\n",
    "x1=np.array(x1)\n",
    "x2=np.array(x2)\n",
    "\n",
    "x0=np.delete(x0,[5,6,7,13,24,25,26,27,28,29]-np.ones(10),axis=1)\n",
    "x0[:,0:1]=number_to_other_number(x0[:,0:1],median0)\n",
    "x0=np.delete(x0,[3,4,7,13,19,20]-np.ones(6),axis=1)\n",
    "x0=min_max_transform(x0,min0,max0)\n",
    "x0[:,index_to_be_skewed0]= np.log(x0[:,index_to_be_skewed0]+1)\n",
    "\n",
    "x1=np.delete(x1,[5,6,7,13,27,28,29]-np.ones(7),axis=1)\n",
    "x1[:,0:1]=number_to_other_number(x1[:,0:1],median1)\n",
    "x1=np.delete(x1,[4,7,19,20]-np.ones(4),axis=1)\n",
    "x1=min_max_transform(x1,min1,max1)\n",
    "x1[:,index_to_be_skewed1]= np.log(x1[:,index_to_be_skewed1]+1)\n",
    "\n",
    "\n",
    "x2=np.delete(x2,[4,6,22,23,24,27,30]-np.ones(7),axis=1)\n",
    "x2[:,0:1]=number_to_other_number(x2[:,0:1],median2)\n",
    "x2=min_max_transform(x2,min2,max2)\n",
    "x2[:,index_to_be_skewed2]= np.log(x2[:,index_to_be_skewed2]+1)\n",
    "\n",
    "\n",
    "x0= build_poly(x0, min_degree0)\n",
    "x0[:,1:]= (x0[:,1:]-mean0)/std0\n",
    "\n",
    "x1= build_poly(x1, min_degree1)\n",
    "x1[:,1:]= (x1[:,1:]-mean1)/std1\n",
    "\n",
    "x2= build_poly(x2, min_degree2)\n",
    "x2[:,1:]= (x2[:,1:]-mean2)/std2\n",
    "\n",
    "\n",
    "y0Predict=predict_labels(w0,x0)\n",
    "y1Predict=predict_labels(w1,x1)\n",
    "y2Predict=predict_labels(w2,x2)\n",
    "\n",
    "y_pred=np.empty(tX_test.shape[0])\n",
    "y_pred[y0indices]=y0Predict\n",
    "y_pred[y1indices]=y1Predict\n",
    "y_pred[y2indices]=y2Predict\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
