{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y,tx,w):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Square Error of the given paramaters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features (N,D)\n",
    "    w : np.array\n",
    "        The weights of the model (D,)\n",
    "\n",
    "    Returns:\n",
    "        The result of the MSE calculation\n",
    "    \"\"\"\n",
    "# Calculate the error\n",
    "    e = y - tx.dot(w)\n",
    "# Calculate the Mean Squared Error\n",
    "    mse=1/2*np.mean(e**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Least squares regression using normal equations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features  (N,D)\n",
    "\n",
    "    Returns:\n",
    "        (w, loss) the last weight vector of the calculation, and the corresponding loss value (cost function).\n",
    "\n",
    "    \"\"\"\n",
    "#Calculate the weight through the normal equation solution\n",
    "    gram_matrix=tx.T@tx\n",
    "    w= np.linalg.solve(gram_matrix, tx.T@y)\n",
    "    loss=mse(y,tx,w)\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Calculate the Ridge Regression using normal equations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features (N,D)\n",
    "    lambda_ : np.float64\n",
    "        Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        (w, loss) the last weight vector of the calculation, and the corresponding loss value (cost function).\n",
    "    \"\"\"\n",
    "    gram_matrix=np.dot(tx.T,tx)\n",
    "    lambda_prime=lambda_*2*len(y)\n",
    "    loss=mse(y,tx,w)\n",
    "    w=np.linalg.solve(gram_matrix + (lambda_prime*np.identity(tx.shape[1])),tx.T@y)\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Compute the gradient\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features (N,D)\n",
    "    w : np.array\n",
    "        The weights of the model (D,)\n",
    "\n",
    "    Returns:\n",
    "        The gradient \n",
    "    \"\"\"\n",
    "def compute_gradient(y, tx, w):\n",
    "    \n",
    "    e = y-np.dot(tx,w)\n",
    "    return -np.dot(tx.T,e)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Linear regression using gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features  (N,D)\n",
    "    initial_w : np.array\n",
    "        Initial random weights of the model (D,)\n",
    "    max_iters: int\n",
    "        The maximum number of iterations\n",
    "    gamma: float\n",
    "        The step size\n",
    "\n",
    "    Returns:\n",
    "        (w, loss) the last weight vector of the calculation, and the corresponding loss value (cost function).\n",
    "\n",
    "    \"\"\"\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        gradient=compute_gradient(y, tx, w)\n",
    "        w=w-gamma*gradient\n",
    "        loss=compute_loss(y, tx, w)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "#         ????????????????ASK?????????????????\n",
    "    #Instead taking the last weight of th max_iters, we take the one corresponding to the smallest \n",
    "    #loss because of the case where the gamma steps over the real minimum we find\n",
    "    min_index=np.argmin(losses)\n",
    "    loss=losses[min_index]\n",
    "    w=ws[min_index]\n",
    "\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "        \n",
    "        Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features  (N,D)\n",
    "    batch_size : int\n",
    "        Number of samples of the batch\n",
    "    num_batches: int\n",
    "        The number of batches (default is 1)\n",
    "    shuffle: bool\n",
    "        Randomize the dataset (default is True)\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Linear regression using stochastic gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        Array of labels (N,)\n",
    "    tx : np.array\n",
    "        Array of the features  (N,D)\n",
    "    initial_w : np.array\n",
    "        Initial random weights of the model (D,)\n",
    "    batch_size : int\n",
    "        Number of samples of the batch\n",
    "    max_iters: int\n",
    "        The maximum number of iterations\n",
    "    gamma: float\n",
    "        The step size\n",
    "\n",
    "    Returns:\n",
    "        (w, loss) the last weight vector of the calculation, and the corresponding loss value (cost function).\n",
    "    \"\"\"\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size,num_batches=1):\n",
    "            gradient=compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w=w-gamma*gradient\n",
    "            loss=compute_loss(minibatch_y, minibatch_tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "#         ????????????????ASK?????????????????\n",
    "    #Instead taking the last weight of th max_iters, we take the one corresponding to the smallest \n",
    "    #loss because of the case where the gamma steps over the real minimum we find \n",
    "    min_index=np.argmin(losses)\n",
    "    loss=losses[min_index]\n",
    "    w=ws[min_index]\n",
    "    return w,loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
